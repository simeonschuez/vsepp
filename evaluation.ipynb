{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from vocab import Vocabulary\n",
    "import evaluation\n",
    "#evaluation.evalrank(\"$RUN_PATH/coco_vse++/model_best.pth.tar\", data_path=\"$DATA_PATH\", split=\"test\")\n",
    "evaluation.evalrank(\"data/runs/coco_vse++/model_best.pth.tar\", data_path=\"data/data/\", split=\"test\")\n",
    "#evaluation.evalrank(\"data/runs/coco_vse++_resnet_restval/model_best.pth.tar\", data_path=\"data/data/\", split=\"test\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import json\n",
    "with open('data/data/coco/annotations/captions_val2014.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "len(data['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy\n",
    "from data import get_test_loader\n",
    "import time\n",
    "import numpy as np\n",
    "from vocab import Vocabulary  # NOQA\n",
    "import torch\n",
    "from model import VSE, order_sim\n",
    "from collections import OrderedDict\n",
    "\n",
    "from evaluation import encode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2i(images, captions, npts=None, return_ranks=False):\n",
    "    \"\"\"\n",
    "    Text->Images (Image Search)\n",
    "    Images: (5N, K) matrix of images\n",
    "    Captions: (5N, K) matrix of captions\n",
    "    \"\"\"\n",
    "    if npts is None:\n",
    "        npts = int(images.shape[0] / 5)\n",
    "        print(npts)\n",
    "    ims = numpy.array([images[i] for i in range(0, len(images), 5)])\n",
    "\n",
    "    ranks = numpy.zeros(5 * npts)\n",
    "    \n",
    "    top1 = numpy.zeros(5 * npts)\n",
    "    for index in range(npts):\n",
    "\n",
    "        # Get query captions\n",
    "        queries = captions[5 * index:5 * index + 5]\n",
    "\n",
    "        # Compute scores\n",
    "        d = numpy.dot(queries, ims.T)\n",
    "        inds = numpy.zeros(d.shape)\n",
    "        for i in range(len(inds)):\n",
    "            inds[i] = numpy.argsort(d[i])[::-1]\n",
    "            ranks[5 * index + i] = numpy.where(inds[i] == index)[0][0]\n",
    "            top1[5 * index + i] = inds[i][0]\n",
    "\n",
    "    # Compute metrics\n",
    "    r1 = 100.0 * len(numpy.where(ranks < 1)[0]) / len(ranks)\n",
    "    r5 = 100.0 * len(numpy.where(ranks < 5)[0]) / len(ranks)\n",
    "    r10 = 100.0 * len(numpy.where(ranks < 10)[0]) / len(ranks)\n",
    "    medr = numpy.floor(numpy.median(ranks)) + 1\n",
    "    meanr = ranks.mean() + 1\n",
    "    if return_ranks:\n",
    "        return (r1, r5, r10, medr, meanr), (ranks, top1)\n",
    "    else:\n",
    "        return (r1, r5, r10, medr, meanr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path=\"data/runs/coco_vse++/model_best.pth.tar\"\n",
    "model_path=\"data/runs/coco_vse++_resnet_restval/model_best.pth.tar\"\n",
    "data_path=\"data/data/\"\n",
    "split=\"test\"\n",
    "vocab_path=None\n",
    "on_gpu=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/runs/coco_vse++_resnet_restval/model_best.pth.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-915ed0b69d86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mon_gpu\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/runs/coco_vse++_resnet_restval/model_best.pth.tar'"
     ]
    }
   ],
   "source": [
    "device = 'cpu' if not on_gpu else 'cuda'\n",
    "checkpoint = torch.load(model_path, map_location=torch.device(device))\n",
    "opt = checkpoint['opt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(Diters=5, Gimage_size=32, Giters=1, batch_size=128, beta1=0.9, beta2=0.999, betas=(0.9, 0.999), clamp_lower=-0.01, clamp_upper=0.01, cnn_type='resnet152', crop_size=224, data_name='coco', embed_size=1024, eta=1.0, eta_m=1.0, finetune=False, gamma=0.1, grad_clip=2.0, img_dim=4096, learning_rate=0.0002, log_step=10, logger_name='runs/coco_uvs_resnet_restval_l2norm', lr_update=15, margin=0.2, max_violation=True, measure='cosine', model_name='UVS', model_path='./model/', ndf=64, no_imgnorm=False, no_prel2norm=False, noadam=False, nol2norm=False, num_epochs=30, num_layers=1, resume='', save_step=1000, txt_dim=6000, use_abs=False, use_mask=False, use_restval=True, val_step=500, vocab_path='./data/', vocab_size=11755, word_dim=300, workers=10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet152'\n",
      "Loading dataset\n",
      "loading annotations into memory...\n",
      "0:00:00.197571\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "if data_path is not None:\n",
    "    opt.data_path = data_path\n",
    "\n",
    "if vocab_path is not None:\n",
    "    opt.vocab_path = vocab_path\n",
    "\n",
    "# load vocabulary used by the model\n",
    "with open(os.path.join(opt.vocab_path,\n",
    "                       '%s_vocab.pkl' % opt.data_name), 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "opt.vocab_size = len(vocab)\n",
    "\n",
    "# construct model\n",
    "model = VSE(opt)\n",
    "\n",
    "# load model state\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "print('Loading dataset')\n",
    "data_loader = get_test_loader(split, opt.data_name, vocab, opt.crop_size,\n",
    "                              opt.batch_size, opt.workers, opt, \n",
    "                              image_location='/home/simeon/Dokumente/Code/Uni/Repos/Adaptive/data/images/mscoco/val2014/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, targets, lengths, ids) in enumerate(data_loader):\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_id = data_loader.dataset.ids[0]\n",
    "caption = data_loader.dataset.coco[0].anns[ann_id]['caption']\n",
    "img_id = data_loader.dataset.coco[0].anns[ann_id]['image_id']\n",
    "path = data_loader.dataset.coco[0].loadImgs(img_id)[0]['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing results...')\n",
    "img_embs, cap_embs = encode_data(model, data_loader, on_gpu=on_gpu)\n",
    "print('Images: %d, Captions: %d' %\n",
    "      (img_embs.shape[0] / 5, cap_embs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((33.76, 68.8, 81.02, 3.0, 12.9344),\n",
       " (array([ 0.,  1., 16., ...,  4.,  2., 20.]),\n",
       "  array([  0.,  21., 789., ..., 387., 839., 958.])))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2i(img_embs, cap_embs, return_ranks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average t2i Recall: 61.2\n",
      "Text to image: 33.8 68.8 81.0 3.0 12.9\n"
     ]
    }
   ],
   "source": [
    "# no cross-validation, full evaluation\n",
    "ri, rti = t2i(img_embs, cap_embs, return_ranks=True, npts=1000)\n",
    "ari = (ri[0] + ri[1] + ri[2]) / 3\n",
    "\n",
    "print(\"Average t2i Recall: %.1f\" % ari)\n",
    "print(\"Text to image: %.1f %.1f %.1f %.1f %.1f\" % ri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
